<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <title>Case Study on Preserving Performance with Smaller Actors in Actor-Critic RL</title>

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <link href="../css/template.css" rel="stylesheet">
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation" style="background-color:#34a853;border-color:#34a853">    
      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse navbar-ex1-collapse">
        <ul class="nav navbar-nav">
            <li role="presentation"> <a href="#overview">Overview</a></li>
            <li role="presentation"> <a href="#dataset">Code</a></li>
          <!-- <li role="presentation"> <a href="#code">Code</li> -->
          <li role="presentation"> <a href="#refs">Reference</a> </li>
          <li role="presentation"> <a href="./index.html#contact">Contact</a></li>
        </ul>
      </div>
    </nav>

    <div class="container">

      <div class="home-intro" style="padding: 5% 10%">
        <div class="row">
            <h1 align="center">Honey, I Shrunk The Actor: A Case Study on Preserving Performance with Smaller Actors in Actor-Critic RL</h1>
        </div>

        <div class="project-page">
          <a name="abstract"></a>
          <center><h2>Abstract</h2></center>
          <p class="text-justify">
            Actors and critics in actor-critic reinforcement learning algorithms are functionally separate, yet they often use the same network architectures.
            This case study explores the performance impact of network sizes when considering actor and critic architectures independently. 
            By relaxing the assumption of architectural symmetry, it is often possible for smaller actors to achieve comparable policy performance to their symmetric counterparts. 
            Our experiments show up to 99% reduction in the number of network weights with an average reduction of 77% over multiple actor-critic algorithms on 9 independent tasks.
            Given that reducing actor complexity results in a direct reduction of run-time inference cost, we believe configurations of actors and critics are aspects of actor-critic design that deserve to be considered independently, particularly in resource-constrained applications or when deploying multiple actors simultaneously.
          </p>
          <br>
          <br>
          <center>
          <a href="./AsymmetricAC_CoG.pdf" target="_blank" class="btn btn-danger" role="button">MAIN PAPER</a>
          <a href="./AsymmetricAC_CoG___Extra_Results.pdf" target="_blank" class="btn btn-danger" role="button">Extended Results</a>
          </center>
        </div>
          
        <div class="project-page">
          <a name="overview"></a>
            <center><h2>Overview</h2></center>
            <br>
            <p>
              RL-based algorithms can learn interesting control policies for a wide arraay of tasks but there has been limited prior exploration into how well optimized the network architectures are, for the various problems explored in contemporary RL.
              While investigating the behavior of Actor-Critic RL algorithms, we were curious about just how small actors could get before they fail to learn.
              To begin, we wanted to see if RL agents would be able to learn to solve a simple problem, for which a linear 1-neuron policy would suffice (more details in the paper)
              We discovered that, if one were to restrict architectural exploration with an assumption of symmetry in the actor and critic architectures, this simple problem could not be solved, however, by breaking the symmetry and using a larger critic, the problem becomes solvable.
              This led us to hypothesize that it was likely the critic that required more representational power in learning to understand the environments, while actors could perhaps manage with smaller networks.
            </p>
            <br>
              <center><img width="100%" alt="Solving a simple toy problem" src="following.png"></center>
            <br><br>
            <p>
              To test this hypothesis, we ran over 1k experiments to determine if 4 commonly used actor-critic algorithms (DDPG, TD3, PPO, SAC) would allow for smaller actors by breaking architectural symmetry with the critics.
              We tested these algorithms on a number of <a href="https://gym.openai.com">OpenAI Gym</a> benchmarks as well as games from the <a href="https://pygame-learning-environment.readthedocs.io/en/latest/index.html">PyGame Learning Environment</a> and <a href="https://github.com/Unity-Technologies/ml-agents">Unity's ML-Agents</a> API.
            </p>
            <br>
              <center><img width="100%" alt="Solving a simple toy problem" src="Envs.png"></center>
            <br><br>
            <p>
              Our results showed that it was indeed often possible to train smaller actors while maintaining performance parity.
              Over the 8 tasks tested, we were able to achieve an average actor-network size reduction of 77% with up to 98% on some tasks.
            </p>
            <br>
              <center><img width="100%" alt="Solving a simple toy problem" src="asymmetric_vs_symmetric.png"></center>
            <br><br>

          </div>

          <div class="project-page">
            <a name="dataset"></a>
            <h2>Code</h2>
            <p>
              For the time being, a zipped copy of the code as used for the benchmarks and toy problem is available via the source below.
            </p>
            <br>
            <center>
              <a href="./AC_Asymmetry.zip" target="_blank" class="btn btn-danger" role="button">Code (toy & benchmarks)</a>
            </center>
          </div>

          <div class="project-page">
            <a name="refs"></a>
            <h2>Reference</h2>
            <p class="lead"> If you find this useful in your work please consider citing: </p>
            <div class="highlight">
            <pre> <code> 
              @inproceedings{mysore2021littleActor, 
              title={Honey, I Shrunk The Actor: A Case Study on Preserving Performance with Smaller Actors in Actor-Critic RL},
              author={Siddharth Mysore and Bassel Mabsout and Renato Mancuso and Kate Saenko},
              year={2021},
              }          
            </code> </pre>
            </div>
          </div>

          <div class="project-page">
            <a name="contact"></a>
            <h2>Contact</h2>
            <p>sidmys [at] bu [dot] edu</p>
          </div>
        </div>
      </div>
    </div> <!--container-->

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <!--<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>-->
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <!--<script src="js/bootstrap.min.js"></script>
    <script src="js/toggle.js"></script> -->
  </body> 
</html>
