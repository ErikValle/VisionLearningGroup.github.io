<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <title>RIFT Disentangled Unsupervised Image Translation via Restricted Information Flow</title>
    <link media="all" href="https://metapose.github.io/css.css" type="text/css" rel="StyleSheet">

  </head>
  <body>
<body>
<div id="primarycontent"> 
<center><h1><b>RIFT</b>: </br> Disentangled Unsupervised Image Translation via Restricted Information Flow</h1></center>
<center><h2>
	<a href="https://usmnb.github.io/">Ben Usman*</a>&nbsp;&nbsp;&nbsp;
	<a href="https://cs-people.bu.edu/dbash/">Dina Bashkirova*</a>&nbsp;&nbsp;&nbsp;
	<a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>
	</h2>

	<center><h2>
		Boston University
	</h2></center>
<center><h2>In WACV 2023</h2></center>
<center><h2><strong><a href="https://drive.google.com/file/d/1H6UOP31H4Ofmq06k37nQaZzn279oPxcZ/view?usp=sharing">Paper</a> | <a href="https://drive.google.com/file/d/1SjhYi87deO0ETd0_52U32cXsuL7pYhxT/view?usp=sharing"> Supplementary </a> | <a href="https://github.com/MInner/rift">Code</a> | <a href="https://drive.google.com/file/d/1Y6RQrZg8rxxn3OI8gCcVabvNx0q1tQWB/view?usp=sharing">Poster</a> | <a href="https://www.youtube.com/watch?v=T3qTaNMjg8k"> Video </a> </strong> </h2></center>

<center style="margin-top:1cm;">
    <img src="https://ai.bu.edu/rift/rift-web-image.png" width="500">
</center>
</br>
<div style="font-size:14px"><p align="justify">
We propose a new many-to-many image translation method that infers which attributes are domain-specific <b>from data</b> by constraining information flow through the network using translation honesty losses and a penalty on the capacity of the domain-specific embedding, and does not rely on hard-coded inductive architectural biases.
</p></div>

</br>
<h2 align="center">Abstract</h2>
</br>
<div style="font-size:14px"><p align="justify"> Unsupervised image-to-image translation methods aim to map images from one domain into plausible examples from another domain while preserving the structure shared across two domains. In the many-to-many setting, an additional guidance example from the target domain is used to determine the domain-specific factors of variation of the generated image. In the absence of attribute annotations, methods have to infer which factors of variation are specific to each domain from data during training. In this paper, we show that many state-of-the-art architectures implicitly treat textures and colors as always being domain-specific, and thus fail when they are not. We propose a new method called RIFT that does not rely on such inductive architectural biases and instead infers which attributes are domain-specific vs shared directly from data. As a result, RIFT achieves consistently high cross-domain manipulation accuracy across multiple datasets spanning a wide variety of domain-specific and shared factors of variation. </p></div>

<iframe width="560" height="315" src="https://www.youtube.com/embed/T3qTaNMjg8k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</br>
</br>
</br>
<h2 align="center">Citation</h2>
<pre align="left"><code align="left">
@inproceedings{usman2023rift,
    author    = {Usman, Ben and Bashkirova, Dina and Saenko, Kate},
    title     = {{RIFT}: Disentangled Unsupervised Image Translation via Restricted Information Flow},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2023}
}
</pre></code>
</br></br></br></br>
</body>
